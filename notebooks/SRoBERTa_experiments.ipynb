{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings (already checked they are not problematic, due to internal checks)\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set gpu: which one to use and memory growth \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Metal device set to:  Physical GPUs, 1 Logical GPUs\n",
      "Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 11:26:22.364572: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-03 11:26:22.364671: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets are configured in data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kps_train = pd.read_csv('../dataset_KPA_2021/kps_train.csv').to_numpy().reshape(-1)\n",
    "args_train = pd.read_csv('../dataset_KPA_2021/args_train.csv').to_numpy().reshape(-1)\n",
    "labels_train = pd.read_csv('../dataset_KPA_2021/labels_train.csv').to_numpy().reshape(-1)\n",
    "\n",
    "kps_dev = pd.read_csv('../dataset_KPA_2021/kps_dev.csv').to_numpy().reshape(-1)\n",
    "args_dev = pd.read_csv('../dataset_KPA_2021/args_dev.csv').to_numpy().reshape(-1)\n",
    "labels_dev = pd.read_csv('../dataset_KPA_2021/labels_dev.csv').to_numpy().reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we only use bert uncased, in particular the base verision is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 250 # we checked in the data analysis notebook that no sentence (arg or kp) is longer than 250 tokens\n",
    "\n",
    "tokenized_args_train = tokenizer(args_train.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "tokenized_kps_train = tokenizer(kps_train.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "\n",
    "tokenized_args_dev = tokenizer(args_dev.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "tokenized_kps_dev = tokenizer(kps_dev.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "(zipping input with desired output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "sizes of data: training (20635, 250) (20635, 250) (20635, 250) (20635, 250)\n",
      "validation/dev data\n",
      "sizes of data: validation (3458, 250) (3458, 250) (3458, 250) (3458, 250)\n"
     ]
    }
   ],
   "source": [
    "## TRAINING \n",
    "toks1_input_train = tokenized_args_train.input_ids\n",
    "atts1_input_train = tokenized_args_train.attention_mask\n",
    "\n",
    "toks2_input_train = tokenized_kps_train.input_ids\n",
    "atts2_input_train = tokenized_kps_train.attention_mask\n",
    "\n",
    "print('training data')\n",
    "print('sizes of data: training', toks1_input_train.shape,atts1_input_train.shape,toks2_input_train.shape,atts2_input_train.shape)\n",
    "\n",
    "\n",
    "## VALIDATION/DEV\n",
    "toks1_input_dev = tokenized_args_dev.input_ids\n",
    "atts1_input_dev = tokenized_args_dev.attention_mask\n",
    "\n",
    "toks2_input_dev = tokenized_kps_dev.input_ids\n",
    "atts2_input_dev = tokenized_kps_dev.attention_mask\n",
    "\n",
    "print('validation/dev data')\n",
    "print('sizes of data: validation', toks1_input_dev.shape,atts1_input_dev.shape,toks2_input_dev.shape,atts2_input_dev.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenization by decoding the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>`people reach their limit when it comes to their quality of life and should be able to end their \n",
      "<s>Assisted suicide gives dignity to the person that wants to commit it</s><pad><pad><pad><pad><pad>\n",
      "0.5\n",
      "<s>a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.</s\n",
      "<s>Assisted suicide allows people to solicit someone to die to their own benefit</s><pad><pad><pad><\n",
      "0.5\n",
      "<s>a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.</s\n",
      "<s>Assisted suicide is akin to killing someone</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  print( tokenizer.decode(toks1_input_train[i])[0:100] ) \n",
    "  print( tokenizer.decode(toks2_input_train[i])[0:100] )\n",
    "  print(labels_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>A real education is about giving students the tools to learn, think, and express themselves; dict\n",
      "<s>School uniform is harming the student's self expression</s><pad><pad><pad><pad><pad><pad><pad><pa\n",
      "1.0\n",
      "<s>A real education is about giving students the tools to learn, think, and express themselves; dict\n",
      "<s>School uniforms are expensive</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
      "0.5\n",
      "<s>A real education is about giving students the tools to learn, think, and express themselves; dict\n",
      "<s>School uniforms are often uncomfortable/sexist</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><p\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  print( tokenizer.decode(toks1_input_dev[i])[0:100] ) \n",
    "  print( tokenizer.decode(toks2_input_dev[i])[0:100] )\n",
    "  print(labels_dev[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = (\n",
    "   toks1_input_train,\n",
    "   atts1_input_train,\n",
    "   toks2_input_train,\n",
    "   atts2_input_train\n",
    "  )\n",
    "input_dataset_train = tf.data.Dataset.from_tensor_slices( inputs_train )\n",
    "output_dataset_train = tf.data.Dataset.from_tensor_slices( labels_train )\n",
    "dataset_train = tf.data.Dataset.zip( (input_dataset_train, output_dataset_train) )\n",
    "dataset_train = dataset_train.shuffle(buffer_size=20635, reshuffle_each_iteration=True, seed=0)\n",
    "\n",
    "inputs_dev = (\n",
    "   toks1_input_dev,\n",
    "   atts1_input_dev,\n",
    "   toks2_input_dev,\n",
    "   atts2_input_dev\n",
    "  )\n",
    "input_dataset_dev = tf.data.Dataset.from_tensor_slices( inputs_dev )\n",
    "output_dataset_dev = tf.data.Dataset.from_tensor_slices( labels_dev )\n",
    "dataset_dev = tf.data.Dataset.zip( (input_dataset_dev, output_dataset_dev) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran an initial grid search on a total of ~80 configurations to find interesting intervals in hyperparaters, then we tested the best intervals on 5 dataset initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from utils import grid_search_iteration\n",
    "\n",
    "embeddings = 'roberta-base'\n",
    "\n",
    "# get previously computed results to append the new ones in the same file\n",
    "try:\n",
    "  results = pd.read_csv('results.csv')\n",
    "  results = results.to_numpy().reshape(-1).tolist()\n",
    "  print(results)\n",
    "except FileNotFoundError:\n",
    "  results = []\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for num_epochs in [1]:\n",
    "  for lr in [(4e-06, 3e-06), (4e-06, 2e-06), (4e-06, 1e-06), (3e-06, 3e-06), (3e-06, 2e-06), (3e-06, 1e-06)]:\n",
    "    # chose the configuration\n",
    "    config = {\n",
    "    'num_epochs' : num_epochs,\n",
    "    'lr' : lr\n",
    "    } \n",
    "    \n",
    "    res_grid_iteration =  grid_search_iteration(config, dataset_train, batch_size, inputs_dev, emb=embeddings)\n",
    "\n",
    "    # save precision togheter with config\n",
    "    results.append(res_grid_iteration)\n",
    "    pd.DataFrame(results).to_csv('./results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-grained grid search with averaged precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from utils import grid_search_iteration\n",
    "\n",
    "embeddings = 'roberta-base'\n",
    "\n",
    "# get previously computed results to append the new ones in the same file\n",
    "try:\n",
    "  results_fine_grained = pd.read_csv('./results_fine_grained.csv')\n",
    "  results_fine_grained = results_fine_grained.to_numpy().reshape(-1).tolist()\n",
    "  print(results_fine_grained)\n",
    "except FileNotFoundError:\n",
    "  results_fine_grained = []\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for num_epochs in [1]:\n",
    "  for lr in [(6e-06, 6e-06)]:\n",
    "    # chose the configuration\n",
    "    config = {\n",
    "    'num_epochs' : num_epochs,\n",
    "    'lr' : lr\n",
    "    } \n",
    "    \n",
    "    res_grid_iteration =  grid_search_iteration(config, dataset_train, batch_size, inputs_dev, num_tests = 5, emb=embeddings)\n",
    "\n",
    "    # save precision togheter with config\n",
    "    results_fine_grained.append(res_grid_iteration)\n",
    "    pd.DataFrame(results_fine_grained).to_csv('./results_fine_grained.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc93c98838970e2501022137642cf0d49080853d19222b3ff97ea24582302a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
