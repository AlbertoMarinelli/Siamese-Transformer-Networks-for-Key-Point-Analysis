{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings (already checked they are not problematic, due to internal checks)\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set gpu: which one to use and memory growth \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets are configured in data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kps_train = pd.read_csv('../dataset_KPA_2021/kps_train.csv').to_numpy().reshape(-1)\n",
    "args_train = pd.read_csv('../dataset_KPA_2021/args_train.csv').to_numpy().reshape(-1)\n",
    "labels_train = pd.read_csv('../dataset_KPA_2021/labels_train.csv').to_numpy().reshape(-1)\n",
    "\n",
    "kps_dev = pd.read_csv('../dataset_KPA_2021/kps_dev.csv').to_numpy().reshape(-1)\n",
    "args_dev = pd.read_csv('../dataset_KPA_2021/args_dev.csv').to_numpy().reshape(-1)\n",
    "labels_dev = pd.read_csv('../dataset_KPA_2021/labels_dev.csv').to_numpy().reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we only use bert uncased, in particular the base verision is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 250 # we checked in the data analysis notebook that no sentence (arg or kp) is longer than 250 tokens\n",
    "\n",
    "tokenized_args_train = tokenizer(args_train.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "tokenized_kps_train = tokenizer(kps_train.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "\n",
    "tokenized_args_dev = tokenizer(args_dev.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "tokenized_kps_dev = tokenizer(kps_dev.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "(zipping input with desired output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING \n",
    "toks1_input_train = tokenized_args_train.input_ids\n",
    "atts1_input_train = tokenized_args_train.attention_mask\n",
    "\n",
    "toks2_input_train = tokenized_kps_train.input_ids\n",
    "atts2_input_train = tokenized_kps_train.attention_mask\n",
    "\n",
    "print('training data')\n",
    "print('sizes of data: training', toks1_input_train.shape,atts1_input_train.shape,toks2_input_train.shape,atts2_input_train.shape)\n",
    "\n",
    "\n",
    "## VALIDATION/DEV\n",
    "toks1_input_dev = tokenized_args_dev.input_ids\n",
    "atts1_input_dev = tokenized_args_dev.attention_mask\n",
    "\n",
    "toks2_input_dev = tokenized_kps_dev.input_ids\n",
    "atts2_input_dev = tokenized_kps_dev.attention_mask\n",
    "\n",
    "print('validation/dev data')\n",
    "print('sizes of data: validation', toks1_input_dev.shape,atts1_input_dev.shape,toks2_input_dev.shape,atts2_input_dev.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenization by decoding the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  print( tokenizer.decode(toks1_input_train[i])[0:100] ) \n",
    "  print( tokenizer.decode(toks2_input_train[i])[0:100] )\n",
    "  print(labels_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  print( tokenizer.decode(toks1_input_dev[i])[0:100] ) \n",
    "  print( tokenizer.decode(toks2_input_dev[i])[0:100] )\n",
    "  print(labels_dev[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = (\n",
    "   toks1_input_train,\n",
    "   atts1_input_train,\n",
    "   toks2_input_train,\n",
    "   atts2_input_train\n",
    "  )\n",
    "input_dataset_train = tf.data.Dataset.from_tensor_slices( inputs_train )\n",
    "output_dataset_train = tf.data.Dataset.from_tensor_slices( labels_train )\n",
    "dataset_train = tf.data.Dataset.zip( (input_dataset_train, output_dataset_train) )\n",
    "dataset_train = dataset_train.shuffle(buffer_size=20635, reshuffle_each_iteration=True, seed=0)\n",
    "\n",
    "inputs_dev = (\n",
    "   toks1_input_dev,\n",
    "   atts1_input_dev,\n",
    "   toks2_input_dev,\n",
    "   atts2_input_dev\n",
    "  )\n",
    "input_dataset_dev = tf.data.Dataset.from_tensor_slices( inputs_dev )\n",
    "output_dataset_dev = tf.data.Dataset.from_tensor_slices( labels_dev )\n",
    "dataset_dev = tf.data.Dataset.zip( (input_dataset_dev, output_dataset_dev) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran an initial grid search on a total of ~80 configurations to find interesting intervals in hyperparaters, then we tested the best intervals on 5 dataset initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from utils import grid_search_iteration\n",
    "\n",
    "# get previously computed results to append the new ones in the same file\n",
    "try:\n",
    "  results = pd.read_csv('results.csv')\n",
    "  results = results.to_numpy().reshape(-1).tolist()\n",
    "  print(results)\n",
    "except FileNotFoundError:\n",
    "  results = []\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for cls_token_activate in [False]:\n",
    "  for num_epochs in [1]:\n",
    "    for lr in [(4e-06, 2e-06), (4e-06, 1e-06), (3e-06, 3e-06), (3e-06, 2e-06), (3e-06, 1e-06)]:\n",
    "      # chose the configuration\n",
    "      config = {\n",
    "      'cls_token_activate' : cls_token_activate,\n",
    "      'num_epochs' : num_epochs,\n",
    "      'lr' : lr\n",
    "      } \n",
    "      \n",
    "      res_grid_iteration =  grid_search_iteration(config, dataset_train, batch_size, inputs_dev)\n",
    "\n",
    "      # save precision togheter with config\n",
    "      results.append(res_grid_iteration)\n",
    "      pd.DataFrame(results).to_csv('./results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-grained grid search with averaged precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from utils import grid_search_iteration\n",
    "\n",
    "# get previously computed results to append the new ones in the same file\n",
    "try:\n",
    "  results_fine_grained = pd.read_csv('./results_fine_grained.csv')\n",
    "  results_fine_grained = results_fine_grained.to_numpy().reshape(-1).tolist()\n",
    "  print(results_fine_grained)\n",
    "except FileNotFoundError:\n",
    "  results_fine_grained = []\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for cls_token_activate in [False]:\n",
    "  for num_epochs in [1]:\n",
    "    for lr in [(3e-06, 2e-06), (3e-06, 1e-06)]:\n",
    "      # chose the configuration\n",
    "      config = {\n",
    "      'cls_token_activate' : cls_token_activate,\n",
    "      'num_epochs' : num_epochs,\n",
    "      'lr' : lr\n",
    "      } \n",
    "      \n",
    "      res_grid_iteration =  grid_search_iteration(config, dataset_train, batch_size, inputs_dev, num_tests = 5)\n",
    "\n",
    "      # save precision togheter with config\n",
    "      results_fine_grained.append(res_grid_iteration)\n",
    "      pd.DataFrame(results_fine_grained).to_csv('./results_fine_grained.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66edcd4fb8f45a7841d6efd802e19caaa33b9d362c458fe42b2899dee719ec37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
